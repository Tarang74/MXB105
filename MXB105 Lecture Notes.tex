%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros
\usepackage{multicol}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\csch}{csch}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\arcsec}{arcsec}
\DeclareMathOperator{\arccsc}{arccsc}

% Header and footer
\newcommand{\unitName}{Calculus and Differential Equations}
\newcommand{\unitTime}{Semester 2, 2021}
\newcommand{\unitCoordinator}{Dr Vivien Challis}
\newcommand{\documentAuthors}{\textsc{Rohan Boas} \quad\quad \textsc{Tarang Janawalkar}}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
%
\tableofcontents
\newpage
%
\section{Analysis of Functions}
\subsection{Functions}
\begin{definition}[Function]
    A function \(f: X \to Y\) is a relation between a set of inputs
    \(X\) and outputs \(Y\), which assigns exactly one output to each
    input. The set \(X\) is called the \textbf{domain} of \(f\), and the
    set \(Y\) is called the \textbf{codomain} of \(f\). Often, the set
    of possible values that \(f\) can take is restricted to a
    \textit{subset} of the codomain, called the \textbf{image}
    (or range) of \(f\).
\end{definition}
As an example, consider the function \(f: \R \to \R\) defined by
\(f\left( x \right) = x^2\). The domain of this function is \(\R\),
the codomain is \(\R\), and the image is \(\R^+\), as the square of any
real number is always positive.
\subsection{Limits}
\begin{definition}[Limit of a function]
    A limit is the value a function approaches as the input approaches
    some value. Limits are used to define concepts in calculus and
    analysis, such as continuity, differentiation, and integration.
    The limit of the function \(f\left( x \right)\) is written as
    \begin{equation*}
        \lim_{x\to x_0} f\left( x \right) = L,
    \end{equation*}
    which means that as \(x\) approaches \(x_0\), the value of
    \(f\left( x \right)\) approaches \(L\). We can alternatively denote
    this using right arrows:
    \begin{equation*}
        f\left( x \right) \to L \text{ as } x \to x_0.
    \end{equation*}
\end{definition}
\begin{definition}[\(\varepsilon\)-\(\delta\) definition of a limit]
    The function \(f: I \to \R\) tends to \(L\) as \(x\) tends to
    \(x_0\) if for every \(\varepsilon > 0\), there exists a
    \(\delta > 0\) such that for all \(x\) in the domain \(I\),
    \(0 < \abs*{x - x_0} < \delta\) implies
    \(\abs*{f\left( x \right) - L} < \varepsilon\):
    \begin{equation*}
        \lim_{x\to x_0} f\left( x \right) = L \iff \forall\varepsilon>0: \exists\delta>0: \forall x \in I: 0<\abs*{x-x_0}<\delta \implies \abs*{f\left( x \right)-L}<\varepsilon
    \end{equation*}
    This is known as the \(\varepsilon\)-\(\delta\) definition of a
    limit, formalised by Cauchy and Weierstrass.
\end{definition}
For example,
\begin{equation*}
    \lim_{x \to 2} 2 x = 4
\end{equation*}
because for every \(\varepsilon > 0\), we can take \(\delta = \varepsilon/2\), so that
\begin{equation*}
    \left( 0 < \abs*{x - 2} < \delta \iff \abs*{x - 2} < \frac{\varepsilon}{2} \right) \implies \abs*{2x - 4} < \varepsilon
\end{equation*}
\begin{definition}[Left- and right-hand limits]
    Left- and right-hand limits characterise the behaviour of a function
    as the input approaches a certain value from either the left or
    right, respectively.
    The left-hand limit of the function \(f\left( x \right)\) as \(x\)
    approaches \(x_0\) is the value of the function as \(x\) approaches
    \(x_0\) from the left, denoted \(\lim_{x\to x_0^-} f\left( x
    \right)\). The right-hand limit of the function \(f\left( x
    \right)\) as \(x\) approaches \(x_0\) is the value of the function
    as \(x\) approaches \(x_0\) from the right, denoted \(\lim_{x\to
    x_0^+} f\left( x \right)\).
\end{definition}
\begin{theorem}[Existence of a limit]
    The function \(f\left( x \right)\) approaches \(L\) as \(x\)
    approaches \(x_0\) if and only if the left- and right-hand limits
    of this function exist and are equal:
    \begin{equation*}
        \lim_{x \to x_0} f\left( x \right) = L \iff \lim_{x \to x_0^-} f\left( x \right) = \lim_{x \to x_0^+} f\left( x \right) = L
    \end{equation*}
\end{theorem}
\begin{theorem}[L'HÃ´pital's Rule]
    For two differentiable functions \(f\left( x \right)\) and \(g\left( x \right)\).
    If \(\displaystyle \lim_{x\to x_0}f\left( x \right)=\lim_{x\to x_0}g\left( x \right)=0\)
    or \(\displaystyle \lim_{x\to x_0}f\left( x \right)=\displaystyle \lim_{x\to x_0}g\left( x \right)=\pm\infty\),
    then
    \begin{equation*}
        \lim_{x\to x_0}\frac{f\left( x \right)}{g\left( x \right)} = \lim_{x\to x_0}\frac{f'\left( x \right)}{g'\left( x \right)}.
    \end{equation*}
\end{theorem}
\subsection{Continuity}
A function is continuous if it does not have any jumps in its graph. In
other words, a small variation in the input also results in a small
variation in the output. Functions that do not satisfy this property
for some value of \(x\) are discontinuous at that point.
\begin{theorem}[Continuity at a point]
    The function \(f\left( x \right)\) is continuous at \(c\) if and
    only if
    \begin{equation*}
        \lim_{x\to c} f\left( x \right) = f\left( c \right).
    \end{equation*}
\end{theorem}
\begin{theorem}[Continuity over an interval]
    The function \(f\left( x \right)\) is continuous on the interval
    \(I\) if it is continuous for all values of \(x \in I\). If the two
    endpoints of this interval are \(a\) and \(b\), then
    \begin{itemize}
        \item \(f\left( x \right)\) is continuous on \(\ointerval{a}{b}\)
              if it is continuous for all \(x\in \ointerval{a}{b}\).
        \item \(f\left( x \right)\) is continuous on \(\interval{a}{b}\)
              if it is continuous for all \(x\in \ointerval{a}{b}\), but
              only right-continuous at \(a\) and left-continuous at \(b\).
              That is, the right-hand limit at \(a\) and the left-hand
              limit at \(b\).
    \end{itemize}
    When \(f\left( x \right)\) is continuous on \(\ointerval{-\infty}{\infty}\),
    it is said to be continuous \textbf{everywhere}.
\end{theorem}
\begin{theorem}[Intermediate value theorem]
    If the function \(f\left( x \right)\) is continuous on
    \(I:\interval{a}{b}\) and \(c\) is any number between
    \(f\left( a \right)\) and \(f\left( b \right)\), inclusive, then
    there exists an \(x\in I\) such that \(f\left( x \right)=c\).
\end{theorem}
\subsection{Differentiability}
Differentiability is a property of functions that relates to the
existence of a derivative at each point on some interval. A valuable
starting point for understanding differentiability is the idea of the
\textit{average rate of change} of a function \(f\left( x \right)\) on
some small interval \(\ointerval{x_0}{x_0+h}\) (where \(h\) is small),
which is given by:
\begin{equation*}
    \left. \adv{f}{x} \right\vert_{x = x_0} = \frac{\text{change in output}}{\text{change in input}} = \frac{f\left( x_0+h \right)-f\left( x_0 \right)}{\left( x_0 + h \right) - x_0} = \frac{f\left( x_0+h \right)-f\left( x_0 \right)}{h}.
\end{equation*}
The last result is known as a difference quotient.
\begin{definition}[Differentiablity at a point]
    A function \(f\left( x \right)\) is differentiable at \(x=x_0\) if
    the limit
    \begin{equation*}
        \lim_{x\to x_0} \frac{f\left( x \right)-f\left( x_0 \right)}{x-x_0}
    \end{equation*}
    exists. When this limit exists, it defines the derivative
    \begin{equation*}
        \left.\odv{f}{x}\right|_{x=x_0} = \lim_{h\to 0} \frac{f\left( x_0+h \right)-f\left( x_0 \right)}{h}.
    \end{equation*}
    The derivative can be thought of as the \textit{instantaneous}
    rate of change of the function \(f\left( x \right)\) at the point
    \(x_0\).
\end{definition}
\begin{definition}[Differentiability on an interval]
    The function \(f\left( x \right)\) is differentiable on an interval
    \(I\) if \(f\left( x \right)\) is differentiable for all \(x_0\in I\).
\end{definition}
\begin{theorem}
    A function that is differentiable at a point is also continuous at
    that point. In other words, differentiability implies continuity.
\end{theorem}
\begin{theorem}[Mean value theorem]
    If the function \(f\left( x \right)\) is differentiable on
    \(I:\interval{a}{b}\) (and therefore also continuous on \(I\)), then
    there exists a point \(c\in I\) where
    \begin{equation*}
        \left.\odv{f}{x}\right|_{x=c}=\frac{f\left( b \right)-f\left( a \right)}{b-a}
    \end{equation*}
\end{theorem}
\section{Definite Integration}
Definite integrals are used to calculate the signed area of a region
between a function and the \(x\)-axis. Later we will see an important
result that relates integration with differentiation allowing us to
calculate integrals of functions.
\begin{definition}[Definite integration]
    If the function \(f\left( x \right)\) is continuous on an interval
    \(I:\interval{a}{b}\), then the net signed area \(A\) between the
    graph of \(f\left( x \right)\) on the interval \(I\) is expressed as
    \begin{equation*}
        A = \int_a^b f\left( x \right) \odif{x}.
    \end{equation*}
\end{definition}
\begin{tcolorboxlarge}[title={Properties of Definite Integrals}]
    Suppose that the functions \(f\left( x \right)\) and \(g\left( x \right)\)
    are continuous on the interval \(I:\interval{a}{b}\), with
    \(a,\:b,\:c\in I\), with \(a < c < b\), and \(k\in\R\) is some
    constant. Then,
    \begin{enumerate}[label=\normalfont\alph*)]
        \item \(\displaystyle\int_a^a f\left( x \right) \odif{x} = 0\).
        \item \(\displaystyle\int_a^b f\left( x \right) \odif{x} = -\int_b^a f\left( x \right) \odif{x}\).
        \item \(\displaystyle\int_a^b kf\left( x \right) \odif{x} = k\int_a^b f\left( x \right) \odif{x}\).
        \item \(\displaystyle\int_a^b \bigl(f\left( x \right) \pm g\left( x \right)\bigr) \odif{x} = \int_a^b f\left( x \right) \odif{x} \pm \int_a^b g\left( x \right) \odif{x}\).
        \item \(\displaystyle\int_a^b f\left( x \right) \odif{x} = \int_a^c f\left( x \right) \odif{x} + \int_c^b f\left( x \right) \odif{x}\).
    \end{enumerate}
\end{tcolorboxlarge}
\subsection{Riemann Sums}
\begin{theorem}
    \label{theorem:1d_riemann_sums}
    The net signed area \(A\) under the function \(f\left( x \right)\)
    on the interval \(\interval{a}{b}\) can be approximated by a Riemann
    sum:
    \begin{equation*}
        A \approx \sum_{k=1}^n f\left( x_k \right) \Delta x_k
    \end{equation*}
    where \(n\) is the number of rectangles, \(x_k\) is the centre of
    the rectangle \(k\), and \(\Delta x_k\) is the width of the
    rectangle \(k\). Note these rectangles do not need to be of equal
    width. In the limit where the widest rectangle's width approaches zero
    (\(\max{\Delta x_k}\to 0\)), the Riemann sum approaches the definite
    integral:
    \begin{equation*}
        A = \int_a^b f\left( x \right) \odif{x} = \lim_{\max{\Delta x_k}\to 0} \sum_{k=1}^n f\left( x_k \right) \Delta x_k
    \end{equation*}
    where, as a consequence, the number of rectangles \(n\) also
    approaches infinity.
    In the case where every rectangle has the same width, we can express
    the width of each rectangle as
    \begin{equation*}
        \forall k:\Delta x_k = \frac{b-a}{n}.
    \end{equation*}
\end{theorem}
\subsection{Fundamental Theorem of Calculus}
The fundamental theorem of calculus provides a logical connection
between infinite series (definite integrals) and antiderivatives
(indefinite integrals).
\begin{theorem}[The Fundamental Theorem of Calculus: Part 1]
    If \(f\left( x \right)\) is continuous on \(\left[ a,\:b \right]\) and \(F\) is
    any antiderivative of \(f\) on \(\left[ a,\:b \right]\) then
    \begin{equation*}
        \int_a^b f\left( x \right)\odif{x} = F\left( b \right) - F\left( a \right)
    \end{equation*}
    Equivalently
    \begin{equation*}
        \int_a^b \odv{F\left( x \right)}{x} \odif{x} = F\left( b \right) - F\left( a \right) \equiv \left.F(x)\right|_a^b
    \end{equation*}
\end{theorem}
\begin{theorem}[The Fundamental Theorem of Calculus: Part 2]
    If \(f\left( x \right)\) is continuous on \(I\) then it has an antiderivative on
    \(I\). In particular, if \(a\in I\), then the function \(F\) defined
    by
    \begin{equation*}
        F\left( x \right) = \int_a^x f\left( t \right)\odif{t}
    \end{equation*}
    is an antiderivative of \(f\left( x \right)\). That is,
    \begin{equation*}
        \odv{F\left( x \right)}{x} = f\left( x \right) \equiv \odv*{\left( \int_a^x f\left( t \right) \odif{t} \right)}{x} = f\left( x \right)
    \end{equation*}
\end{theorem}
\begin{theorem}
    Differentiation and integration are inverse operations.
\end{theorem}
\newpage
\subsection{Taylor and Maclaurin Polynomials}
\begin{theorem}[Taylor Polynomials]
    If \(f\left( x \right)\) is an \(n\) differentiable function at \(x_0\), then the
    \(n\)th degree Taylor polynomial for \(f\left( x \right)\) near \(x_0\), is given
    by
    \begin{equation*}
        f\left( x \right) \approx p_n\left( x \right) = \sum_{k=0}^n \frac{f^{\left( k \right)}\left( x_0 \right)}{k!} \left( x-x_0 \right)^k
    \end{equation*}
\end{theorem}
\begin{theorem}[Maclaurin Polynomials]
    Evaluating a Taylor polynomial near \(0\), gives the \(n\)th degree
    Maclaurin polynomial for \(f\left( x \right)\)
    \begin{equation*}
        f\left( x \right) \approx p_n\left( x \right) = \sum_{k=0}^n \frac{f^{\left( k \right)}\left( 0 \right)}{k!} x^k
    \end{equation*}
\end{theorem}
\begin{theorem}[Error in Approximation]
    Let \(R_n\left( x \right)\) denote the difference between \(f\left( x \right)\) and its
    \(n\)th Taylor polynomial, that is
    \begin{equation*}
        R_n\left( x \right) = f\left( x \right) - p_n\left( x \right) = f\left( x \right) - \sum_{k=0}^n \frac{f^{\left( k \right)}\left( x_0 \right)}{k!} \left( x-x_0 \right)^k = \frac{f^{\left( n+1 \right)}\left( s \right)}{\left( n+1 \right)!} \left( x-x_0 \right)^{n+1}
    \end{equation*}
    where \(s\) is between \(x_0\) and \(x\).
\end{theorem}
\newpage
\section{Taylor and Maclaurin Series}
\subsection{Infinite Series}
\begin{definition}[Taylor Series]
    If \(f\left( x \right)\)  has derivatives of all orders at \(x_0\), then the
    Taylor series for \(f\left( x \right)\) about \(x=x_0\) is given by
    \begin{equation*}
        f\left( x \right) = \sum_{n=0}^{\infty} \frac{f^{\left( n \right)}\left( x_0 \right)}{n!}\left( x-x_0 \right)^n
    \end{equation*}
\end{definition}
\begin{definition}[Maclaurin Series]
    If a Taylor series is centred at \(x_0=0\), it is called a Maclaurin
    series, defined by
    \begin{equation*}
        f\left( x \right) = \sum_{n=0}^{\infty} \frac{f^{\left( n \right)}\left( 0 \right)}{n!} x^n
    \end{equation*}
\end{definition}
\begin{definition}[Power Series]
    Both Taylor and Maclaurin series are examples of \textbf{power
        series}, which are defined as follows
    \begin{equation*}
        \sum_{n=0}^{\infty} c_n\left( x-x_0 \right)^n
    \end{equation*}
\end{definition}
\subsection{Convergence}
\begin{theorem}[Convergence of a Taylor Series]
    The equality
    \begin{equation*}
        f\left( x \right) = \sum_{n=0}^{\infty} \frac{f^{\left( n \right)}\left( x_0 \right)}{n!} \left( x-x_0 \right)^n
    \end{equation*}
    holds at a point \(x\) iff
    \begin{align*}
        \lim_{n\to\infty} \left[ f\left( x \right) - \sum_{k=0}^{n} \frac{f^{\left( k \right)}\left( x_0 \right)}{k!} \left( x-x_0 \right)^k \right] & = 0 \\
        \lim_{n\to\infty} R_n\left( x \right)                                                                                                        & = 0
    \end{align*}
\end{theorem}
\begin{definition}[Interval of Convergence]
    The interval of convergence for a power series is the set of \(x\)
    values for which that series converges.
\end{definition}
\begin{definition}[Radius of Convergence]
    The radius of convergence \(R\) is a non-negative real number or
    \(\infty\) such that a power series converges if
    \begin{equation*}
        \abs*{x - a} < R
    \end{equation*}
    and diverges if
    \begin{equation*}
        \abs*{x - a} > R
    \end{equation*}
    The behaviour of the power series on the boundary, that is, where
    \(\abs*{x - a} = R\), can be determined by substituting \(x = R + a\)
    and \(x = -R + a\) into the series, for the upper and lower
    boundaries, respectively.
\end{definition}
\subsection{Convergence Tests}
For any power series of the form \(\displaystyle\sum_{i=i_0}^\infty
a_i\), the following tests can be used to determine the radius of
convergence.
\begin{tcolorboxlarge}[title={Alternating Series}]
    \textbf{Conditions} \(a_i = \left( -1 \right)^i b_i\) or
    \(a_i = \left( -1 \right)^{i+1} b_i\). \(b_i>0\).
    \begin{equation*}
        \text{Is \(b_{i+1}\leqslant b_i\) \& \(\lim_{i\to\infty}b_i=0\)?}\:
        \begin{cases}
            \text{YES} & \text{\(\sum a_i\) Converges} \\
            \text{NO}  & \text{Inconclusive}
        \end{cases}
    \end{equation*}
\end{tcolorboxlarge}
\begin{tcolorboxlarge}[title={Ratio Test}]
    \begin{equation*}
        \text{Is \(\lim_{i\to\infty}\abs*{\frac{a_{i+1}}{a_i}} < 1\)?}\:
        \begin{cases}
            \text{YES} & \text{\(\sum a_i\) Converges} \\
            \text{NO}  & \text{\(\sum a_i\) Diverges}
        \end{cases}
    \end{equation*}
    The ratio test is inconclusive if
    \(\displaystyle \lim_{i\to\infty}\abs*{\frac{a_{i+1}}{a_i}} = 1\).
\end{tcolorboxlarge}
\subsection{Table of Maclaurin Series}
\begin{table}[H]
    \centering
    \begin{tabular}{c c c}
        \toprule
        \textbf{Function}                         & \textbf{Series}                                                                                 & \textbf{Interval of Convergence} \\
        \midrule
        \(e^{x}\)                                 & \(\displaystyle \sum_{n=0}^{\infty} \frac{x^n}{n!}\)                                            & \(-\infty < x < \infty\)         \\[14pt]
        \(\sin{\left( x \right)}\)                & \(\displaystyle \sum_{n=0}^{\infty} \left( -1 \right)^n \frac{x^{2n+1}}{\left( 2n+1 \right)!}\) & \(-\infty < x < \infty\)         \\[14pt]
        \(\cos{\left( x \right)}\)                & \(\displaystyle \sum_{n=0}^{\infty} \left( -1 \right)^n \frac{x^{2n}}{\left( 2n \right)!}\)     & \(-\infty < x < \infty\)         \\[14pt]
        \(\displaystyle \frac{1}{1-x}\)           & \(\displaystyle \sum_{n=0}^{\infty} x^n\)                                                       & \(-1 < x < 1\)                   \\[14pt]
        \(\displaystyle \frac{1}{1+x^2}\)         & \(\displaystyle \sum_{n=0}^{\infty} \left( -1 \right)^n x^{2n}\)                                & \(-1 < x < 1\)                   \\[14pt]
        \(\displaystyle \ln{\left( 1+x \right)}\) & \(\displaystyle \sum_{n=0}^{\infty} \left( -1 \right)^{n+1} \frac{x^n}{n}\)                     & \(-1 < x \leq 1\)                \\[14pt]
        \bottomrule
    \end{tabular}
    \caption{Maclaurin series of common functions.}
    % \label{}
\end{table}
\newpage
\section{Multivariable Calculus}
\subsection{Multivariable Functions}
\begin{definition}
    A function is multivariable if its domain consists of several
    variables. In the reals, these functions are defined
    \begin{equation*}
        f:\R^n\to\R
    \end{equation*}
\end{definition}
\subsection{Level Curves}
\begin{definition}
    The level curves or \textit{contour curves} of a function of two
    variables are curves along which the function has a constant value.
    \begin{equation*}
        L_c\left( f \right) = \bigl\{ \left( x,\: y \right) : f\left(x,\: y\right) = c\bigr\}
    \end{equation*}
    The level curves of a function can be determined by substituting
    \(z = c\), and solving for \(y\).
\end{definition}
\subsection{Limits and Continuity}
\begin{definition}[Finite Limit of Multivariable Functions using the
        \(\varepsilon\)-\(\delta\) Definition]
    \begin{align*}
        \lim_{\left( x_1,\: \ldots,\: x_n \right)\to \left( c_1,\: \ldots,\: c_n \right)} f\left( x_1,\: \ldots,\: x_n \right) = L
                                                    & \iff\forall\varepsilon>0: \exists\delta>0: \forall \left( x_1,\: \ldots,\: x_n \right) \in I: \\
        0<\abs*{x_1-c_1,\: \ldots,\: x_n-c_n}<\delta & \implies \abs*{f\left( x_1,\: \ldots,\: x_n \right)-L}<\varepsilon
    \end{align*}
\end{definition}
\begin{theorem}[Limits along Smooth Curves]
    If \(f\left( x,\: y \right) \to L\) as \(\left( x,\: y \right) \to \left( x_0,\: y_0 \right)\), then
    \(\displaystyle \lim_{\left( x,\: y \right) \to \left( x_0,\: y_0 \right)} = L\) along any
    smooth curve.
\end{theorem}
\begin{theorem}[Existence of a Limit]
    If the limit of \(f\left( x,\: y \right)\) changes along different smooth curves,
    then \(\displaystyle \lim_{\left( x,\: y \right) \to \left( x_0,\: y_0 \right)}\) does not
    exist.
\end{theorem}
\begin{theorem}[Continuity of Multivariable Functions]
    A function \(f\left( x_1,\: \ldots,\: x_n \right)\) is continuous at
    \(\left( c_1,\: \ldots,\: c_n \right)\) iff
    \begin{equation*}
        \lim_{\left( x_1,\: \ldots,\: x_n \right)\to \left( c_1,\: \ldots,\: c_n \right)} f\left( x_1,\: \ldots,\: x_n \right) = f\left( c_1,\: \ldots,\: c_n \right)
    \end{equation*}
\end{theorem}
Recognising continuous functions:
\begin{itemize}
    \item A sum, difference or product of continuous functions is
          continuous.
    \item A quotient of continuous functions is continuous except where
          the denominator is zero.
    \item A composition of continuous functions is continuous.
\end{itemize}
\subsection{Partial Derivatives}
\begin{definition}[Partial Differentiation]
    The partial derivative of a multivariable function is its derivative
    with respect to one of those variables, while the others are held
    constant.
    \begin{equation*}
        \pdv{f}{x_i} = \lim_{h \to 0} \frac{f\left( x_1,\: \ldots,\: x_{i-1},\: x_i+h,\: x_{i+1},\: \ldots,\: x_n \right) - f\left( x_1,\: \ldots,\: x_n \right)}{h}
    \end{equation*}
\end{definition}
\subsection{The Gradient Vector}
\begin{definition}
    Let \(\symbf{\nabla}\), pronounced ``del'', denote the vector
    differential operator defined as follows
    \begin{equation*}
        \symbf{\nabla} =
        \begin{bmatrix}
            \pdif*{x_1} \\
            \pdif*{x_2} \\
            \vdots      \\
            \pdif*{x_n}
        \end{bmatrix}
    \end{equation*}
\end{definition}
\subsection{Multivariable Chain Rule}
\begin{definition}
    Let \(f=f\bigl(\symbf{x}\left( t_1,\: \ldots,\: t_n \right)\bigr)\) be the
    composition of \(f\) with \(\symbf{x} =
    \begin{bmatrix}
        x_1 & \cdots & x_m
    \end{bmatrix}
    \),
    then the partial derivative of \(f\) with respect to \(t_i\) is
    given by
    \begin{equation*}
        \pdv{f}{t_i} = \symbf{\nabla}f \cdot \pdv{\symbf{x}}{t_i}
    \end{equation*}
\end{definition}
\subsection{Directional Derivatives}
\begin{definition}
    The directional derivative \(\symbf{\nabla}_{\symbf{u}}f\) is the
    rate at which the function \(f\) changes in the direction
    \(\symbf{u}\).
    \begin{align*}
        \symbf{\nabla}_{\symbf{u}}f & = \lim_{h \to 0} \frac{f\left( \symbf{x} + h\symbf{u} \right) - f\left( \symbf{x} \right)}{h} \\
                                    & = \symbf{\nabla}f \cdot \symbf{u}
    \end{align*}
    where the slope is given by \(\norm{\symbf{\nabla}_{\symbf{u}}f}\).
\end{definition}
\begin{remark}
    The directional derivative of \(f\) can be denoted in several ways:
    \begin{equation*}
        \symbf{\nabla}_{\symbf{u}}f = \pdv{f}!{\symbf{u}} = \pdv{f}{\symbf{u}}
    \end{equation*}
\end{remark}
\begin{theorem}[Direction of Greatest Ascent]
    The direction of greatest ascent is given by
    \begin{equation*}
        \max_{\norm{\symbf{u}} = 1} \symbf{\nabla}_{\symbf{u}}f = \symbf{\nabla}f
    \end{equation*}
    where the slope is given by \(\norm{\symbf{\nabla}f}\).
\end{theorem}
\begin{theorem}[Direction of Greatest Descent]
    The direction of greatest descent is given by
    \begin{equation*}
        \min_{\norm{\symbf{u}} = 1} \symbf{\nabla}_{\symbf{u}}f = -\symbf{\nabla}f
    \end{equation*}
    where the slope is \(-\norm{\symbf{\nabla}f}\).
\end{theorem}
\begin{proof}
    Given that \(\symbf{u}\) is a unit vector, the dot product
    definition gives
    \begin{align}
        \symbf{\nabla}_{\symbf{u}}f & = \symbf{\nabla}f \cdot \symbf{u} \nonumber                                         \\
                                    & = \norm{\symbf{\nabla}f} \norm{\symbf{u}} \cos{\left( \theta \right)} \nonumber     \\
                                    & = \norm{\symbf{\nabla}f} \cos{\left( \theta \right)} \label{directional_derivative}
    \end{align}
    \hyperref[directional_derivative]{Equation~\ref{directional_derivative}}
    is maximised when \(\cos{\left( \theta \right)}\) is maximised. Thus,
    the maximum slope is
    \begin{equation*}
        \max \symbf{\nabla}_{\symbf{u}}f = \norm{\symbf{\nabla}f}
    \end{equation*}
    and the direction of greatest ascent is
    \begin{equation*}
        \symbf{u} = \symbf{\nabla}f
    \end{equation*}
\end{proof}
\begin{theorem}
    If \(\symbf{\nabla}f\left( c_1,\: \ldots,\: c_n \right) \neq 0\), then \(\symbf{\nabla}f\left( c_1,\: \ldots,\: c_n \right)\) is normal to the level
    curve of \(f\) that passes through \(\left( c_1,\: \ldots,\: c_n \right)\).
\end{theorem}
\subsection{Higher-Order Partial Derivatives}
\begin{definition}
    Higher-order partial derivatives can be denoted using three
    different notations. The following table shows the mixed partial
    derivative of \(f\left( x,\: y \right)\) w.r.t. \(x\) then \(y\).
    \begin{table}[H]
        \centering
        \begin{tabular}{c c c}
            \toprule
            \textbf{Leibniz}              & \textbf{Euler}    & \textbf{Legendre} \\
            \midrule
            \(\displaystyle\pdv{f}{y,x}\) & \(\pdv{f}!{y,x}\) & \(f_{x y}\)       \\
            \bottomrule
        \end{tabular}
        \caption{Mixed Partial Derivative Notation}
    \end{table}
    For partial derivatives w.r.t.\ the same variable, a superscript can
    be used in Leibniz and Euler notation.
    \begin{table}[H]
        \centering
        \begin{tabular}{c c c}
            \toprule
            \textbf{Leibniz}               & \textbf{Euler}                        & \textbf{Legendre} \\
            \midrule
            \(\displaystyle\pdv[2]{f}{x}\) & \(\displaystyle\pdv[order=2]{f}!{x}\) & \(f_{x x}\)       \\
            \bottomrule
        \end{tabular}
        \caption{Second-Order Partial Derivative Notation}
        % \label{}
    \end{table}
\end{definition}
\subsection{Hessian Matrix}
\begin{definition}
    Let the Hessian matrix \(\symbf{H}\) be the matrix of second-order
    partial derivative operators as shown below
    \begin{equation*}
        \symbf{H} =
        \begin{bmatrix}
            \pdif*[order=2]{x_1} & \cdots & \pdif*{x_n,x_1}      \\
            \vdots               & \ddots & \vdots               \\
            \pdif*{x_1,x_n}      & \cdots & \pdif*[order=2]{x_n}
        \end{bmatrix}
    \end{equation*}
    Operating on the function \(f\left( x,\: y \right)\) gives
    \begin{equation*}
        \symbf{H}_f =
        \begin{bmatrix}
            \pdv[order=2]{f}!{x_1} & \cdots & \pdv{f}!{x_n,x_1}      \\
            \vdots                 & \ddots & \vdots                 \\
            \pdv{f}!{x_1,x_n}      & \cdots & \pdv[order=2]{f}!{x_n}
        \end{bmatrix}
    \end{equation*}
\end{definition}
\subsection{Critical Points}
For the function \(f\left( x,\: y \right)\), the point \(\left( x_0,\:
y_0 \right)\) is a critical point if
\begin{equation*}
    \symbf{\nabla}f\left( x_0,\: y_0 \right) = 0
\end{equation*}
or if \(\symbf{\nabla}f\left( x_0,\: y_0 \right)\) is undefined.
\subsection{Classification of Critical Points}
The nature of a critical point can be classified using the second
derivative test:
\begin{itemize}
    \item if \(\left.\det{\left(\mathbf{H}_f\right)}\right|_{(x_0,\:
          y_0)} > 0\), then the point is a local minima or maxima
          \begin{itemize}
              \item if \(f_{xx}\left( x_0, \: y_0 \right) < 0\), then
                    the point is a local maxima
              \item if \(f_{xx}\left( x_0, \: y_0 \right) > 0\), then
                    the point is a local minima\footnote{For the local
                    minima/maxima, the second derivative can also be
                    taken w.r.t. \(y\).}
          \end{itemize}
    \item if \(\left.\det{\left(\mathbf{H}_f\right)}\right|_{(x_0,\:
          y_0)} < 0\), then the point is a saddle point
    \item if \(\left.\det{\left(\mathbf{H}_f\right)}\right|_{(x_0,\:
          y_0)} = 0\), then the test is inconclusive.
\end{itemize}
\newpage
\section{Double and Triple Integrals}
\subsection{Volume under a Two Variable Function}
\begin{definition}
    If \(f\) is a function of two variables that is continuous and
    non-negative on a region \(\Omega\) in the \(xy\)-plane, then the
    volume of the solid enclosed between the surface \(z=f\left( x,\: y \right)\) and
    the region \(\Omega\) is defined by
    \begin{equation}
        \label{eq:volume_under_2d_function}
        V = \lim_{n \to \infty} \sum_{k = 1}^n f\left( x_k^\ast,\: y_k^\ast \right) \Delta A_k
    \end{equation}
\end{definition}
\begin{proof}
    Using lines parallel to the coordinate axes, the region \(\Omega\)
    can be divided into \(n\) rectangles, where any rectangles outside
    \(\Omega\) are discarded. The area of the \(k\)th remaining
    rectangle at the arbitrary point \(\left( x_k^\ast,\: y_k^\ast \right)\) is given
    by \(\Delta A_k\). Thus, the product
    \(f\left( x_k^\ast,\: y_k^\ast \right)\Delta A_k\) is the volume of the \(k\)th
    rectangular parallelepiped, and the sum of all \(n\) volumes over
    the region \(\Omega\) approximate the volume \(V\) of the entire
    solid.
\end{proof}
\subsection{Double Integral}
\begin{definition}
    By extension of the definite integral of a single variable function
    expressed in
    \hyperref[theorem:1d_riemann_sums]{Theorem~\ref{theorem:1d_riemann_sums}},
    the sums in
    \hyperref[eq:volume_under_2d_function]{Equation~\ref{eq:volume_under_2d_function}}
    are also called Riemann sums, and the limit is denoted as
    \begin{equation*}
        \iint\limits_{\Omega} f\left( x,\: y \right)  \odif{A}
        = \sum_{k=1}^{\infty} f\left( x_k^\ast,\: y_k^\ast \right) \Delta A_k
    \end{equation*}
\end{definition}
\begin{tcolorboxlarge}[title={Properties of Double Integrals}]
    \begin{theorem}
        Suppose that \(f\left( x,\: y \right) \) and \(g\left( x,\: y \right)\) are continuous on
        \(\Omega\), and \(\Omega\) can be subdivided into \(\Omega_1\)
        and \(\Omega_2\), then
        \begin{enumerate}[label=\normalfont\alph*)]
            \item \(\displaystyle\iint\limits_\Omega kf\left( x,\: y \right) \odif{A}
                  = k\iint\limits_\Omega f\left( x,\: y \right) \odif{A}\).
            \item \(\displaystyle\iint\limits_\Omega f\left( x,\: y \right) + g\left( x,\: y \right) \odif{A}
                  = \iint\limits_\Omega f\left( x,\: y \right) \odif{A} + \iint\limits_\Omega g\left( x,\: y \right) \odif{A}\).
            \item \(\displaystyle\iint\limits_\Omega f\left( x,\: y \right) \odif{A}
                  = \iint\limits_{\Omega_1} f\left( x,\: y \right) \odif{A} + \iint\limits_{\Omega_2} f\left( x,\: y \right) \odif{A}\).
        \end{enumerate}
    \end{theorem}
\end{tcolorboxlarge}
\subsection{Rectangular Regions}
If \(\Omega\) is a region bounded by \(a \leq x \leq b\) and \(c \leq y
\leq d\), then
\begin{equation*}
    \iint\limits_{\Omega} f\left( x,\: y \right) \odif{A} = \int_c^d\int_a^b f\left( x,\: y \right) \odif{x} \odif{y} = \int_a^b\int_c^d f\left( x,\: y \right) \odif{y} \odif{x}
\end{equation*}
\subsection{Non-rectangular Regions}
If the limits of integration depend on the variable \(x\) or \(y\),
then the region may be classified as Type I or Type II\@.
\subsubsection{Type I Regions}
If the region is:
\begin{description}
    \item[Bounded on the left \& right by:] \(x=a\) and \(x=b\)
    \item[Bounded below \& above by:] \(y=g_1\left( x \right)\) and
          \(y=g_2\left( x \right)\)
\end{description}
where \(g_1\left( x \right) \leq g_2\left( x \right)\) for \(a \leq x \leq b\), then
\begin{equation*}
    \iint\limits_{\Omega} f\left( x,\: y \right) \odif{A} = \int_a^b\int_{g_1\left( x \right)}^{g_2\left( x \right)} f\left( x,\: y \right) \odif{y} \odif{x}.
\end{equation*}
\subsubsection{Type II Regions}
If the region is:
\begin{description}
    \item[Bounded on the left \& right by:] \(x=h_1\left( y \right)\)
          and \(x=h_2\left( y \right)\)
    \item[Bounded below \& above by:] \(y=c\) and \(y=d\)
\end{description}
where \(h_1\left( y \right) \leq h_2\left( y \right)\) for \(c \leq y \leq d\), then
\begin{equation*}
    \iint\limits_{\Omega} f\left( x,\: y \right) \odif{A} = \int_c^d\int_{h_1\left( y \right)}^{h_2\left( y \right)} f\left( x,\: y \right) \odif{x} \odif{y}.
\end{equation*}
\subsection{Polar Coordinates}
To determine the area of a region defined using polar coordinates, the
function can be integrated w.r.t.\ the radius \(r\), and the angle
\(\theta\).
\begin{equation*}
    \iint\limits_{\Omega} f\left( r,\: \theta \right) \odif{A} = \int_{\theta_1}^{\theta_2}\int_{r_1\left( \theta \right)}^{r_2\left( \theta \right)} f\left(r,\: \theta\right) r \odif{r} \odif{\theta}.
\end{equation*}
\subsection{Volume of a Three Variable Function}
\begin{definition}
    If \(f\) is a function of three variables that is continuous and
    non-negative on a region \(\Omega\) in the \(xyz\)-space, then the
    volume enclosed by \(f\left( x,\: y, \: z \right)\) and the region \(\Omega\) is
    defined by
    \begin{equation}
        \label{eq:volume_of_3d_function}
        V = \lim_{n\to\infty} \sum_{k=1}^{n} f\left( x_k^\ast,\: y_k^\ast,\: z_k^\ast \right) \Delta V_k
    \end{equation}
\end{definition}
\begin{proof}
    Using planes parallel to the coordinate planes, the region
    \(\Omega\) can be divided into \(n\) boxes, where boxes containing
    points outside \(\Omega\) are discarded. The volume of the
    \(k\)th remaining box at the arbitrary point
    \(\left( x_k^\ast,\: y_k^\ast,\: z_k^\ast \right)\) is \(\Delta V_k\). Thus, the
    product \(f\left( x_k^\ast,\: y_k^\ast,\: z_k^\ast \right)\Delta V_k\) is the
    volume of the \(k\)th box, and the sum of all \(n\) volumes over the
    region \(\Omega\) approximate the volume \(V\) of the entire solid.
\end{proof}
\subsection{Triple Integrals}
\begin{definition}
    The triple integral of a function is the net signed volume defined
    over a finite closed solid region \(\Omega\), and the sums in
    \hyperref[eq:volume_of_3d_function]{Equation~\ref{eq:volume_of_3d_function}}
    are also called Riemann sums, and the limit is denoted as
    \begin{equation*}
        \iiint\limits_{\Omega} f\left( x,\: y,\: z \right)  \odif{V}
        = \sum_{k=1}^{\infty} f\left( x_k^\ast,\: y_k^\ast,\: z_k^\ast \right) \Delta V_k
    \end{equation*}
\end{definition}
\begin{tcolorboxlarge}[title={Properties of Triple Integrals}]
    \begin{theorem}
        Suppose that \(f\left( x,\: y,\: z \right) \) and \(g\left( x,\: y,\: z \right)\) are
        continuous on \(\Omega\), and \(\Omega\) can be subdivided into
        \(\Omega_1\) and \(\Omega_2\) then
        \begin{enumerate}[label=\normalfont\alph*)]
            \item \(\displaystyle\iiint\limits_\Omega kf\left( x,\: y,\: z \right) \odif{V}
                  = k\iiint\limits_\Omega f\left( x,\: y,\: z \right) \odif{V}\).
            \item \(\displaystyle\iiint\limits_\Omega f\left( x,\: y,\: z \right) + g\left( x,\: y,\: z \right) \odif{V}
                  = \iiint\limits_\Omega f\left( x,\: y,\: z \right) \odif{V} + \iiint\limits_\Omega g\left( x,\: y,\: z \right) \odif{V}\).
            \item \(\displaystyle\iiint\limits_\Omega f\left( x,\: y,\: z \right) \odif{V}
                  = \iiint\limits_{\Omega_1} f\left( x,\: y,\: z \right) \odif{V} + \iiint\limits_{\Omega_2} f\left( x,\: y,\: z \right) \odif{V}\).
        \end{enumerate}
    \end{theorem}
\end{tcolorboxlarge}
\newpage
\section{Vector-Valued Functions}
% Conventions:
% Component-form vectors shall be written like \(\abracket{x\left( t \right),\:y\left( t \right),\:(z\left( t \right)}\)
\begin{definition}
    A vector-valued function (VVF) is some function \(\symbf{r}\) with
    domain \(\R\) and codomain \(\R^n\). For example,
    \begin{equation*}
        \symbf{r}:\R\to\R^3
        :\symbf{r}\left( t \right)=\abracket*{x\left( t \right),\: y\left( t \right),\: z\left( t \right)}
    \end{equation*}
    is a VVF where \(x,\:y,\:z: \R\to\R\).
\end{definition}
\begin{theorem}
    The domain of \(\symbf{r}\left( t \right)\) is the intersection of the domains of
    its components.
\end{theorem}
\begin{definition}[Orientation]
    The orientation of \(\symbf{r}\left( t \right)\) is the direction of motion along
    the curve as the value of the parameter increases.
\end{definition}
\subsection{Limits and Continuity}
\begin{theorem}[Limits of VVFs]
    The limit of a VVF is the vector of the limits of its components.
    \begin{equation*}
        \lim_{t\to a} \symbf{r}\left( t \right)
        = \abracket*{\lim_{t\to a} x\left( t \right),\: \lim_{t\to a} y\left( t \right),\: \lim_{t\to a} z\left( t \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Continuity of VVFs]
    The VVF \(\symbf{r}\left( t \right)\) is continuous at \(t=a\) iff
    \begin{equation*}
        \lim_{t\to a} \symbf{r}\left( t \right) = \symbf{r}\left( a \right).
    \end{equation*}
    This follows that a VVF is continuous if each of its components are
    also continuous.
\end{theorem}
\subsection{Calculus with VVFs}
\begin{theorem}[Derivatives of VVFs]
    The derivative of a VVF is the vector of the derivatives of its
    components.
    \begin{equation*}
        \odv*{\symbf{r}\left( t \right)}{t} = \*{\odv*{x\left( t \right)}{t},\: \odv*{y\left( t \right)}{t},\: \odv*{z\left( t \right)}{t}}
    \end{equation*}
\end{theorem}
\begin{theorem}[Integration of VVFs]
    The integral of a VVF is the vector of the integrals of its
    components.
    \begin{equation*}
        \int \symbf{r}\left( t \right) \odif{t}
        = \abracket*{\int x\left( t \right) \odif{t},\: \int y\left( t \right) \odif{t},\: \int z\left( t \right) \odif{t}}
    \end{equation*}
\end{theorem}
\begin{remark}
    When integrating a VVF, each component has its own constant of
    integration.
\end{remark}
\subsection{Parametrising Lines with VVFs}
\begin{definition}[Equation for a Line]
    A line can be expressed as
    \begin{equation*}
        \symbf{l}\left( t \right) = \symbf{P}_0 + t\symbf{v}
    \end{equation*}
    where the line \(\symbf{l}\left( t \right)\) passes through the point
    \(\symbf{P}_0\), and is parallel to the vector \(\symbf{v}\).
\end{definition}
\begin{definition}[Tangent Lines]
    If a VVF \(\symbf{r}\left( t \right)\) is differentiable at \(t_0\) and
    \(\symbf{r'}\left( t_0 \right)\ne\symbf{0}\), the tangent line at \(t=t_0\) is
    given by
    \begin{equation*}
        \symbf{l}\left( t \right) = \symbf{r}\left( t_0 \right)+t\symbf{r'}\left( t_0 \right).
    \end{equation*}
\end{definition}
\begin{remark}
    Higher-order approximations can be determined using Taylor's formula.
\end{remark}
\subsection{Applications of VVFs}
\begin{theorem}[Curve of Intersection]
    A VVF can be used to determine the curve of intersection between two
    surfaces. The method is to choose one of the variables (commonly the
    first) as the parameter, and express the remaining variables in
    terms of that parameter. If the intersection is bounded between two
    points, the domain can be calculated using the component which was
    parametrised. For example, the curve of intersection between
    \begin{align*}
        y = 2x - 4 \quad \text{and} \quad z = 3x - 1
    \end{align*}
    between the points
    \begin{align*}
        \symbf{P}_1 = \left( 2,\: 0,\: 7 \right) \quad \text{and} \quad \symbf{P}_2 = \left( 3,\: 2,\: 10 \right)
    \end{align*}
    is given by
    \begin{equation*}
        \symbf{r}\left( t \right) = \abracket*{t,\: 2t - 4,\: 3t - 1} : 2 \leq t \leq 3.
    \end{equation*}
\end{theorem}
\begin{definition}[Arc Length]
    The arc length \(S\) of a smooth continuous VVF \(\symbf{r}\left( t \right)\), is
    the distance along \(\symbf{r}\left( t \right)\) between \(t=a\) and \(t=b\),
    defined by
    \begin{equation*}
        S = \int_a^b \norm{\symbf{r'}\left( t \right)} \odif{t}
    \end{equation*}
\end{definition}
\newpage
\section{Differential Equations}
\begin{definition}[Differential Equations]
    A differential equation (DE) is an equation which involves the
    derivatives of one or more unknown functions (called dependent
    variables), that are with respect to one or more independent
    variables.
\end{definition}
\begin{definition}[Ordinary Differential Equations]
    An ordinary differential equation (ODE) is a differential equation
    with derivatives with respect to a single variable.
\end{definition}
\begin{definition}[Partial Differential Equations]
    A partial differential equation (PDE) is a differential equation
    with derivatives with respect to multiple variables.
\end{definition}
\begin{definition}[Order of Differential Equations]
    The order of a differential equation is the highest derivative in
    the equation.
\end{definition}
\begin{definition}[Autonomous Differential Equations]
    An autonomous differential equation does not depend explicitly on
    the independent variable.
\end{definition}
\begin{definition}[Linear Differential Equations]
    A linear differential equation does not have any products of the
    dependent variable with itself or its derivatives. The general form
    of a linear ODE of order \(n\) is
    \begin{equation*}
        a_n\left( x \right)y^{\left( n \right)} + a_{n-1}\left( x \right)y^{\left( n-1 \right)} + \cdots + a_1\left( x \right)y' + a_0\left( x \right)y = F\left( x \right).
    \end{equation*}
    The dependent variable cannot be composed in another function.
\end{definition}
\subsection{Qualitative Analysis}
With qualitative analysis we aim to understand the behaviour of
solutions to the ODE\@. By \linebreak computing fixed points, we can
draw a phase line diagram, and sketch solution curves. For the
autonomous differential equation
\begin{equation*}
    \odv{y}{t} = f\left( y \right)
\end{equation*}
\begin{definition}[Fixed Point]
    A fixed point is the value of \(y\) for which \(f\left( y \right) = 0\).
\end{definition}
\begin{definition}[Stability]
    By analysing the behaviour of \(f\left( y \right)\) given a perturbation near
    fixed points, we can determine the stability of those fixed points.
    \begin{table}[H]
        \centering
        \begin{tabular}{c c}
            \toprule
            \textbf{Behaviour in Positive/Negative Directions} & \textbf{Stability} \\
            \midrule
            Both toward fixed point                            & stable             \\
            Both away from fixed point                         & unstable           \\
            One toward and one away from fixed point           & semi-stable        \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{definition}
\begin{definition}[Phase Plane]
    Using the information about the behaviour of \(f\left( y \right)\) around fixed
    points, \(f\left( y \right)\) can be plotted against \(y\) to construct a phase
    plane diagram.
\end{definition}
\begin{definition}[Phase Line]
    A phase line is the one-dimensional form of a phase plane, that
    shows the limiting behaviour of \(y\) as \(t \to \infty\).
\end{definition}
\begin{definition}[Sample Solutions]
    Using a phase line, we sketch the behaviour of sample solutions of
    \(f\left( y \right)\) where the curves asymptote toward stable (also
    semi-stable) fixed points, and diverge from unstable (also
    semi-stable) fixed points.
\end{definition}
\newpage
\section{First-Order Differential Equations}
\subsection{Directly Integrable ODEs}
For a differential equation of the form
\begin{equation*}
    \odv{y}{x} = f\left( x \right)
\end{equation*}
\begin{equation*}
    y\left( x \right) = \int f\left( x \right) \odif{x}.
\end{equation*}
\subsection{Separable ODEs}
For a differential equation of the form
\begin{equation*}
    \odv{y}{x} = p\left( x \right) q\left( y \right),
\end{equation*}
a separation of variables followed by an integration w.r.t., \(x\)
yields an implicit solution.
\begin{equation*}
    \int \frac{1}{q\left( y \right)} \odv{y}{x} \odif{x} = \int p\left( x \right) \odif{x}.
\end{equation*}
\subsection{Linear ODEs}
For a differential equation of the form
\begin{equation*}
    \odv{y}{x} + p\left( x \right)y = q\left( x \right)
\end{equation*}
we can use the integrating factor
\begin{equation*}
    I\left( x \right) = e^{\int p\left( x \right) \odif{x}}
\end{equation*}
so that
\begin{equation*}
    y\left( x \right) = \frac{1}{I\left( x \right)} \int I\left( x \right) q\left( x \right) \odif{x}
\end{equation*}
\subsection{Exact ODEs}
A differential equation of the form
\begin{equation*}
    P\left( x,\: y \right) + Q\left( x,\: y \right) \odv{y}{x} = 0
\end{equation*}
has the solution
\begin{equation*}
    \Psi\left( x,\: y \right) = c,
\end{equation*}
iff it is exact, namely when
\begin{equation*}
    P_y = Q_x
\end{equation*}
where \(P = \Psi_x\) and \(Q = \Psi_y\). Then
\begin{gather*}
    \Psi\left( x,\: y \right) = \int P\left( x,\: y \right) \odif{x} + f\left( y \right) \\
    \Psi\left( x,\: y \right) = \int Q\left( x,\: y \right) \odif{y} + g\left( x \right)
\end{gather*}
and \(f\left( y \right)\) and \(g\left( x \right)\) can be determined by solving these equations
simultaneously.
\newpage
\section{Second-Order Differential Equations}
A linear second-order differential equation is of the form
\begin{equation*}
    a_2\left( x \right)y'' + a_1\left( x \right)y' + a_0\left( x \right)y = F\left( x \right).
\end{equation*}
\begin{itemize}
    \item If \(F\left( x \right) = 0\), then the equation is
          homogeneous.
    \item If \(F\left( x \right) \neq 0\), then the equation is
          nonhomogeneous.
\end{itemize}
\begin{definition}[Initial Value Problem]
    An initial value problem specifies the value for \(y\) and its
    derivative at a single value of the independent variable:
    \begin{align*}
        y\left( x_0 \right) = y_0, \quad y'\left( x_0 \right) = y_1.
    \end{align*}
\end{definition}
\begin{definition}[Boundary Value Problem]
    A boundary value problem specifies the value for \(y\) at two
    different values of the independent variable:
    \begin{align*}
        y\left( x_0 \right) = y_0, \quad y\left( x_1 \right) = y_1.
    \end{align*}
\end{definition}
\begin{theorem}[Superposition Principle]
    Consider the linear homogeneous ODE
    \begin{equation*}
        a_n\left( x \right)y^{\left( n \right)} + a_{n-1}\left( x \right)y^{\left( n-1 \right)} + \cdots + a_1\left( x \right)y' + a_0\left( x \right)y = 0.
    \end{equation*}
    If \(y_1\left( x \right),\: y_2\left( x \right),\: \dots ,\: y_n\left( x \right)\) are solutions of the
    differential equation, then the linear combination of these
    solutions
    \begin{equation*}
        y\left( x \right) = c_1 y_1\left( x \right) + c_2 y_2\left( x \right) + \cdots + c_n y_n\left( x \right)
    \end{equation*}
    also satisfies the ODE\@.
\end{theorem}
\begin{theorem}[Fundamental Set of Solutions]
    The \(n\)th order linear homogeneous ODE with \linebreak continuous
    coefficients on an open interval \(I\), has \(n\) non-trivial
    linearly independent solutions that form a fundamental set of
    solutions on \(I\).
\end{theorem}
\subsection{Reduction of Order}
Reduction of order is a method for finding a second solution to an ODE,
given a known solution. The second solution is of the form
\begin{equation*}
    y_2\left( x \right) = v\left(x\right) y_1\left( x \right).
\end{equation*}
\(v\left( x \right)\) can be determined by substituting \(y_2\) into the ODE\@.
\subsection{Homogeneous ODEs}
A second-order constant-coefficient homogeneous ODE
\begin{equation*}
    a_2\odv[2]{y}{x} + a_1\odv{y}{x} + a_0y = 0
\end{equation*}
has solutions of the form
\begin{equation*}
    y\left( x \right) = e^{\lambda x}.
\end{equation*}
\subsection{Characteristic Equation}
Substituting this form into the ODE gives the characteristic equation
\begin{equation*}
    a_2\lambda^2 + a_1\lambda + a_0 = 0.
\end{equation*}
This equation has three distinct cases.
\begin{description}
    \item[Real Distinct Roots.] If \(a_1^2 > 4a_0a_2\).
    \item[Real Repeated Roots.] If \(a_1^2 = 4a_0a_2\).
    \item[Complex Conjugate Roots.] If \(a_1^2 < 4a_0a_2\).
\end{description}
\subsubsection{Real Distinct Roots}
For two real and distinct roots, \(\lambda_1\) and \(\lambda_2\), the
general solution is
\begin{equation*}
    y\left( x \right) = c_1e^{\lambda_1 x} + c_2e^{\lambda_2 x}
\end{equation*}
\subsubsection{Real Repeated Roots}
For the real repeated root, \(\lambda\), the general solution is
\begin{equation*}
    y\left( x \right) = c_1e^{\lambda x} + c_2 te^{\lambda x}
\end{equation*}
\subsubsection{Complex Conjugate Roots}
For two complex conjugate roots, \(\lambda = \alpha \pm \beta i\), the
general solution is
\begin{equation*}
    y\left( x \right) = e^{\alpha x}\bigl( c_1\cos{\left( \beta x \right)} + c_2 \sin{\left( \beta x \right)} \bigr)
\end{equation*}
\section{Nonhomogeneous Differential Equations}
A second-order constant-coefficient nonhomogeneous ODE
\begin{equation*}
    a_2y'' + a_1y' + a_0y = F\left( x \right)
\end{equation*}
has a general solution of the form
\begin{equation*}
    y\left( x \right) = y_H\left( x \right) + y_P\left( x \right)
\end{equation*}
where \(y_H\left( x \right)\) satisfies
\begin{equation*}
    a_2\odv[2]{y_H}{x} + a_1\odv{y_H}{x} + a_0y_H = 0
\end{equation*}
and \(y_P\left( x \right)\) satisfies
\begin{equation*}
    a_2\odv[2]{y_P}{x} + a_1\odv{y_P}{x} + a_0y_P = F\left( x \right)
\end{equation*}
\subsection{Method of Undetermined Coefficients}
To find the particular solution \(y_P\), we must choose a likely form
that it would take. The following table summarises appropriate forms of
\(y_P\) based on \(F\).
\begin{table}[H]
    \centering
    \begin{tabular}{c c}
        \toprule
        \(F\left( x \right)\) form                                             & \(y_P\left( x \right)\) guess                                             \\
        \midrule
        a constant                                                             & \(A\)                                                                     \\
        a polynomial of degree \(n\)                                           & \(\displaystyle \sum_{i = 0}^n A_i x^i\)                                  \\
        \(e^{kx}\)                                                             & \(A e^{kx}\)                                                              \\
        \(\cos{\left( \omega x \right)}\) or \(\sin{\left( \omega x \right)}\) & \(A_0 \cos{\left( \omega x \right)} + A_1 \sin{\left( \omega x \right)}\) \\
        \bottomrule
    \end{tabular}
    \caption{Particular solutions for undetermined coefficients.}
    % \label{}
\end{table}
Once the form of the particular solution has been determined, it can be
substituted into the nonhomogeneous ODE, to determine the undetermined
coefficients.
\subsection{Special Forms}
\subsubsection{Product of Forms}
If \(F\left( x \right)\) is a product of the functions shown above,
then the particular solutions are also multiplied together and any
constants are simplified.
\subsubsection{Sum of Forms}
If \(F\left( x \right)\) is a sum of the functions shown above, then
the particular solutions are also added together.
\subsubsection{Linearly Dependent Forms}
If \(F\left( x \right)\) is similar to any homogeneous solution, then
by \textit{definition} of a homogeneous solution, the solution will be
\(0\). Hence, \(y_P\) must be multiplied by \(x\) to ensure that the
particular solution is linearly independent to the homogeneous
solutions, in order to form a \textit{fundamental set of solutions}.
\subsection{Finding the General Solution}
\begin{enumerate}
    \item Solve \(y_H\)
    \item Find an appropriate form for \(y_P\)
    \item Ensure that \(y_P\) is linearly independent to the
          homogeneous solutions
    \item Substitute \(y_P\) into the nonhomogeneous ODE and solve for
          the undetermined coefficients
    \item Find the general solution \(y = y_H + y_P\)
    \item Apply any initial or boundary conditions
\end{enumerate}
\subsection{Applications of Second-Order ODEs}
\subsubsection{Spring and Mass Systems}
\begin{equation*}
    F = \text{spring force (\(F_s\))} + \text{damping force (\(F_d\))} + \text{external force (\(f\left( t \right)\))}
\end{equation*}
\begin{description}
    \item[Newton's Law] \(F = m y''\)
    \item[Spring force.] \(F_s = -k y\)
    \item[Damping force.] \(F_d = -\gamma y'\)
\end{description}
\begin{equation*}
    m y'' + \gamma y' + k y = f\left( t \right)
\end{equation*}
\subsubsection{Electrical Circuits}
Current \(i\) is defined as the rate of change of charge \(q\)
\begin{equation*}
    i = \odv{q}{t}
\end{equation*}
The voltage drop across various elements is given below:
\begin{description}
    \item[Voltage drop across a resistor:] \(iR\)
    \item[Voltage drop across a capacitor:] \(q/C\)
    \item[Voltage drop across an inductor:] \(L \odv{i}{t}\)
\end{description}
where \(R\) is the resistance measured in Ohms (\unit{\Omega}),
\(C\) is the capacitance measured in Farads (\unit{F}),
and \(L\) is the inductance measured in Henrys (\unit{H}).
Kirchhoff's Voltage law states that the sum of voltages around a loop
equals 0. Therefore, in an RLC circuit, with a voltage source supplying
\(v\left( t \right)\unit{V}\),
\begin{align*}
    v\left( t \right) - iR - L \odv{i}{t} - \frac{q}{C} & = 0                  \\
    L \odv[2]{q}{t} + R\odv{q}{t} + \frac{1}{C}q        & = v\left( t \right).
\end{align*}
\begin{appendix}
    \section{Integration Techniques}
    \subsection{Table of Derivatives}
    Let \(f\left( x \right)\) be a function, and \(a\in\R\) be a
    constant.
    \begin{table}[H]
        \renewcommand*{\arraystretch}{1.5}
        \centering
        \begin{tabular}{c c}
            \toprule
            \(f\)                                                              & \(\odv{f}{x}\)                                   \\
            \midrule
            \(x^a\)                                                            & \(a x^{a-1}\)                                    \\
            \(\sqrt{x}\)                                                       & \(\displaystyle \frac{1}{2\sqrt{x}}\)            \\
            \(a^x\)                                                            & \(\ln{\left(a\right)} a^x\)                      \\
            \(e^x\)                                                            & \(e^x\)                                          \\
            \(\log_a{\left(x\right)}, \: a\in \R\backslash\left\{ 0 \right\}\) & \(\displaystyle \frac{1}{x\ln{\left(a\right)}}\) \\[8pt]
            \(\ln{\left(x\right)}\)                                            & \(\displaystyle \frac{1}{x}\)                    \\[5pt]
            \bottomrule
        \end{tabular}
        \begin{tabular}{c c}
            \toprule
            \(f\)                                                         & \(\odv{f}{x}\)                                                            \\
            \midrule
            \(a\)                                                         & \(0\)                                                                     \\
            \(x\)                                                         & \(1\)                                                                     \\
            \(a_1 u\left( x \right) \pm a_2 v\left( x \right)\)           & \(\displaystyle a_1\odv{u}{x} \pm a_2\odv{v}{x}\)                         \\[8pt]
            \(u\left( x \right)v\left( x \right)\)                        & \(\displaystyle \odv{u}{x}v + u\odv{v}{x}\)                               \\[10pt]
            \(\displaystyle \frac{u\left( x \right)}{v\left( x \right)}\) & \(\displaystyle \frac{\odv{u}{x}v - u\odv{v}{x}}{{v\left( x \right)}^2}\) \\[8pt]
            \(u\bigl(v\left(x \right) \bigr)\)                            & \(\displaystyle \odv{u}{v}\odv{v}{x}\)                                    \\[5pt]
            \bottomrule
        \end{tabular}
    \end{table}
    \begin{table}[H]
        \renewcommand*{\arraystretch}{1.5}
        \centering
        \begin{tabular}{c c}
            \toprule
            \(f\)                       & \(\odv{f}{x}\)                                       \\
            \midrule
            \(\sin{\left( ax \right)}\) & \(a\cos{\left( ax \right)}\)                         \\
            \(\cos{\left( ax \right)}\) & \(-a\sin{\left( ax \right)}\)                        \\
            \(\tan{\left( ax \right)}\) & \(a\sec^2{\left( ax \right)}\)                       \\
            \(\cot{\left( ax \right)}\) & \(-a\csc^2{\left( ax \right)}\)                      \\
            \(\sec{\left( ax \right)}\) & \(a\sec{\left( ax \right)}\tan{\left( ax \right)}\)  \\
            \(\csc{\left( ax \right)}\) & \(-a\csc{\left( ax \right)}\cot{\left( ax \right)}\) \\[5pt]
            \bottomrule
        \end{tabular}
        \begin{tabular}{c c}
            \toprule
            \(f\)                          & \(\odv{f}{x}\)                                  \\
            \midrule
            \(\arcsin{\left( ax \right)}\) & \(\displaystyle  \frac{a}{\sqrt{1-a^2x^2}}\)    \\[8pt]
            \(\arccos{\left( ax \right)}\) & \(\displaystyle -\frac{a}{\sqrt{1-a^2x^2}}\)    \\[8pt]
            \(\arctan{\left( ax \right)}\) & \(\displaystyle  \frac{a}{1+a^2x^2}\)           \\[8pt]
            \(\arccot{\left( ax \right)}\) & \(\displaystyle -\frac{a}{1+a^2x^2}\)           \\[8pt]
            \(\arcsec{\left( ax \right)}\) & \(\displaystyle  \frac{1}{x\sqrt{a^2x^2 - 1}}\) \\[8pt]
            \(\arccsc{\left( ax \right)}\) & \(\displaystyle -\frac{1}{x\sqrt{a^2x^2 - 1}}\) \\[8pt]
            \bottomrule
        \end{tabular}
    \end{table}
    \begin{table}[H]
        \renewcommand*{\arraystretch}{1.5}
        \centering
        \hspace*{-1cm}
        \begin{tabular}{c c}
            \toprule
            \(f\)                        & \(\odv{f}{x}\)                                        \\
            \midrule
            \(\sinh{\left( ax \right)}\) & \(a\cosh{\left( ax \right)}\)                         \\
            \(\cosh{\left( ax \right)}\) & \(a\sinh{\left( ax \right)}\)                         \\
            \(\tanh{\left( ax \right)}\) & \(a\sech^2{\left( ax \right)}\)                       \\
            \(\coth{\left( ax \right)}\) & \(-a\csch^2{\left( ax \right)}\)                      \\
            \(\sech{\left( ax \right)}\) & \(-a\sech{\left( ax \right)}\tan{\left( ax \right)}\) \\
            \(\csch{\left( ax \right)}\) & \(-a\csch{\left( ax \right)}\cot{\left( ax \right)}\) \\[5pt]
            \bottomrule
        \end{tabular}
        \begin{tabular}{c c}
            \toprule
            \(f\)                           & \(\odv{f}{x}\)                              \\
            \midrule
            \(\arcsinh{\left( ax \right)}\) & \(\displaystyle \frac{a}{\sqrt{1+a^2x^2}}\) \\[8pt]
            \(\arccosh{\left( ax \right)}\) & \(\displaystyle \frac{a}{\sqrt{1-a^2x^2}}\) \\[8pt]
            \(\arctanh{\left( ax \right)}\) & \(\displaystyle \frac{a}{1-a^2x^2}\)        \\[8pt]
            \bottomrule
        \end{tabular}
        \begin{tabular}{c c}
            \toprule
            \(f\)                           & \(\odv{f}{x}\)                                                            \\
            \midrule
            \(\arccoth{\left( ax \right)}\) & \(\displaystyle  \frac{a}{1-a^2x^2}\)                                     \\[8pt]
            \(\arcsech{\left( ax \right)}\) & \(\displaystyle -\frac{1}{a\left( 1+ax \right)\sqrt{\frac{1-ax}{1+ax}}}\) \\[8pt]
            \(\arccsch{\left( ax \right)}\) & \(\displaystyle -\frac{1}{ax^2\sqrt{1+\frac{1}{a^2x^2}}}\)                \\[8pt]
            \bottomrule
        \end{tabular}
        \hspace*{-1cm}
        \caption{Derivatives of Elementary Functions}
    \end{table}
    \subsection{Trigonometric Identities}
    \subsubsection{Pythagorean Identities}
    \begin{equation*}
        \sin^2{\left( x \right)} + \cos^2{\left( x \right)} = 1
    \end{equation*}
    Dividing by either the sine or cosine function gives:
    \begin{align*}
        \tan^2{\left( x \right)} + 1 & = \sec^2{\left( x \right)} \\
        1 + \cot^2{\left( x \right)} & = \csc^2{\left( x \right)}
    \end{align*}
    \subsubsection{Double-Angle Identities}
    \begin{align*}
        \sin{\left( 2x \right)} & = 2\sin{\left( x \right)}\cos{\left( x \right)}              & \csc{\left( 2x \right)} & = \frac{\sec{\left( x \right)}\csc{\left( x \right)}}{2}                                                     \\[5pt]
        \cos{\left( 2x \right)} & = \cos^2{\left( x \right)} - \sin^2{\left( x \right)}        & \sec{\left( 2x \right)} & = \frac{\sec^2{\left( x \right)}\csc^2{\left( x \right)}}{\csc^2{\left( x \right)}-\sec^2{\left( x \right)}} \\[5pt]
        \tan{\left( 2x \right)} & = \frac{2\tan{\left( x \right)}}{1-\tan^2{\left( x \right)}} & \cot{\left( 2x \right)} & = \frac{\cot^2{\left( x \right)}-1}{2\cot{\left( x \right)}}
    \end{align*}
    \subsubsection{Power Reducing Identities}
    \begin{align*}
        \sin^2{\left( x \right)} & = \frac{1-\cos{\left( 2x \right)}}{2}                         & \csc^2{\left( x \right)} & = \frac{2}{1-\cos{\left( 2x \right)}}                         \\[5pt]
        \cos^2{\left( x \right)} & = \frac{1+\cos{\left( 2x \right)}}{2}                         & \sec^2{\left( x \right)} & = \frac{2}{1+\cos{\left( 2x \right)}}                         \\[5pt]
        \tan^2{\left( x \right)} & = \frac{1-\cos{\left( 2x \right)}}{1+\cos{\left( 2x \right)}} & \cot^2{\left( x \right)} & = \frac{1+\cos{\left( 2x \right)}}{1-\cos{\left( 2x \right)}}
    \end{align*}
    \subsection{Partial Fractions}
    \begin{definition}[Partial Fraction Decomposition]
        \textbf{Partial fraction decomposition} is a \linebreak method where
        a rational function \(\displaystyle \frac{P\left( x \right)}{Q\left( x \right)}\) is rewritten
        as a sum of fraction.
    \end{definition}
    \begin{table}[H]
        \renewcommand*{\arraystretch}{1.5}
        \centering
        \begin{tabular}{c c}
            \toprule
            \textbf{Factor in denominator}            & \textbf{Term in partial fraction decomposition}                                                                                                    \\
            \midrule
            \(ax+b\)                                  & \(\displaystyle \frac{A}{ax+b}\)                                                                                                                   \\[10pt]
            \(\left(ax+b\right)^k, \: k \in \N\)      & \(\displaystyle \frac{A_1}{ax+b} + \frac{A_2}{\left( ax+b \right)^2} + \cdots + \frac{A_k}{\left( ax+b \right)^k}\)                                \\[10pt]
            \(ax^2+bx+c\)                             & \(\displaystyle \frac{A}{ax^2+bx+c}\)                                                                                                              \\[10pt]
            \(\left(ax^2+bx+c\right)^k, \: k \in \N\) & \(\displaystyle \frac{A_1x+B_1}{ax^2+bx+c} + \frac{A_2x+B_2}{\left( ax^2+bx+c \right)^2} + \cdots + \frac{A_k x+B_k}{\left( ax^2+bx+c \right)^k}\) \\[10pt]
            \bottomrule
        \end{tabular}
        \caption{Partial Fraction Forms}
    \end{table}
    \subsection{Integration by Parts}
    \begin{theorem}
        \begin{equation*}
            \int u\left( x \right) \odv{v\left( x \right)}{x} \odif{x} = u\left( x \right) v\left( x \right) - \int v\left( x \right) \odv{u\left( x \right)}{x} \odif{x} \implies \int u \odif{v} = uv - \int v \odif{u}
        \end{equation*}
    \end{theorem}
    \subsection{Integration by Substitution}
    \begin{theorem}
        \begin{equation*}
            \int f\bigl(g\left( x \right)\bigr)\odv{g\left( x \right)}{x} \odif{x} = \int f\left( u \right) \odif{u}, \: \text{where } u = g\left( x \right)
        \end{equation*}
    \end{theorem}
    \subsection{Trigonometric Substitutions}
    \begin{table}[H]
        \renewcommand*{\arraystretch}{1.5}
        \centering
        \begin{tabular}{c c c c}
            \toprule
            \textbf{Form}                 & \textbf{Substitution}                                      & \textbf{Result}                      & \textbf{Domain}                                                                        \\
            \midrule
            \(\left(a^2-b^2x^2\right)^n\) & \(\displaystyle x=\frac{a}{b}\sin{\left( \theta \right)}\) & \(a^2\cos^2{\left( \theta \right)}\) & \(\theta\in \left[ -\frac{\pi}{2},\: \frac{\pi}{2} \right]\)                           \\[8pt]
            \(\left(a^2+b^2x^2\right)^n\) & \(\displaystyle x=\frac{a}{b}\tan{\left( \theta \right)}\) & \(a^2\sec^2{\left( \theta \right)}\) & \(\theta\in \left( -\frac{\pi}{2},\: \frac{\pi}{2} \right)\)                           \\[8pt]
            \(\left(b^2x^2-a^2\right)^n\) & \(\displaystyle x=\frac{a}{b}\sec{\left( \theta \right)}\) & \(a^2\tan^2{\left( \theta \right)}\) & \(\theta\in \left[ 0,\: \frac{\pi}{2} \right) \cup \left(\frac{\pi}{2},\: \pi\right]\) \\
            \bottomrule
        \end{tabular}
        \caption{Trigonometric substitutions for various forms.}
    \end{table}
\end{appendix}
\end{document}

